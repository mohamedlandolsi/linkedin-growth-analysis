"""
Test script for LinkedIn scraper with automatic browser detection
This version automatically finds and uses any available browser on your system
"""

import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent))

from scripts.linkedin_scraper_auto import LinkedInPostScraper, BrowserDetector
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def test_scraper():
    """Test the LinkedIn scraper with automatic browser detection."""
    
    post_url = "https://www.linkedin.com/posts/klarna_klarnas-climate-resilience-program-activity-7346877091532959746-748v/"
    output_path = "data/json/post_data.json"
    
    print("\n" + "="*70)
    print("LINKEDIN POST SCRAPER - AUTO BROWSER DETECTION")
    print("="*70)
    
    # Detect browsers
    print("\nüîç Scanning your system for browsers...")
    available_browsers = BrowserDetector.detect_browsers()
    
    if not available_browsers:
        print("\n‚ùå NO BROWSERS FOUND!")
        print("\nPlease install at least one of these browsers:")
        print("  ‚Ä¢ Google Chrome: https://www.google.com/chrome/")
        print("  ‚Ä¢ Microsoft Edge: (pre-installed on Windows 10/11)")
        print("  ‚Ä¢ Mozilla Firefox: https://www.mozilla.org/firefox/")
        print("  ‚Ä¢ Brave Browser: https://brave.com/")
        print("  ‚Ä¢ Opera: https://www.opera.com/")
        print("\n" + "="*70)
        return
    
    print(f"\n‚úÖ Found {len(available_browsers)} browser(s):")
    for i, browser in enumerate(available_browsers, 1):
        icon = "üåê" if browser == "chrome" else "üî∑" if browser == "edge" else "ü¶ä" if browser == "firefox" else "ü¶Å" if browser == "brave" else "‚≠ï"
        print(f"   {i}. {icon} {browser.upper()}")
    
    print(f"\nüöÄ Will use: {available_browsers[0].upper()}")
    
    print(f"\nüìã Target URL: {post_url}")
    print(f"üíæ Output Path: {output_path}")
    
    print("\n‚ö†Ô∏è  IMPORTANT NOTES:")
    print("  - Browser window will open automatically")
    print("  - If prompted to login, you have 60 seconds to authenticate")
    print("  - Data extraction happens automatically after page loads")
    print("  - Results will be saved to JSON")
    
    print("\n‚è≥ Starting in 3 seconds...")
    print("="*70 + "\n")
    
    import time
    time.sleep(3)
    
    # Create scraper (it will auto-detect and use first available browser)
    scraper = LinkedInPostScraper(headless=False)
    
    try:
        post_data = scraper.scrape_post(post_url, output_path)
        
        print("\n" + "="*70)
        print("‚úÖ SCRAPING COMPLETED SUCCESSFULLY")
        print("="*70)
        
        print(f"\nüåê Browser Used: {post_data.get('browser_used', 'Unknown')}")
        print(f"üìÖ Extracted at: {post_data['extracted_at']}")
        print(f"‚úîÔ∏è  Status: {post_data['extraction_status'].upper()}")
        
        if post_data.get('author'):
            print(f"\nüë§ Author: {post_data['author']}")
        
        if post_data.get('post_text'):
            preview = post_data['post_text'][:200]
            print(f"\nüìÑ Post Text Preview:")
            print(f"   {preview}{'...' if len(post_data['post_text']) > 200 else ''}")
            print(f"   (Total: {len(post_data['post_text'])} characters)")
        
        print(f"\nüìä Engagement Metrics:")
        print(f"   üëç Likes:    {post_data.get('likes', 'Not extracted'):>8}")
        print(f"   üí¨ Comments: {post_data.get('comments', 'Not extracted'):>8}")
        print(f"   üîÑ Shares:   {post_data.get('shares', 'Not extracted'):>8}")
        
        # Calculate total engagement
        if all([post_data.get('likes'), post_data.get('comments'), post_data.get('shares')]):
            total = post_data['likes'] + post_data['comments'] + post_data['shares']
            print(f"   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            print(f"   üìà Total:    {total:>8}")
        
        print(f"\nüíæ Data saved to: {output_path}")
        
        if post_data.get('errors'):
            print(f"\n‚ö†Ô∏è  Warnings/Errors encountered:")
            for error in post_data['errors']:
                print(f"   ‚Ä¢ {error}")
        
        print("\n" + "="*70)
        print("üéâ All done! Check the JSON file for complete data.")
        print("="*70 + "\n")
        
        return post_data
        
    except Exception as e:
        print(f"\n‚ùå ERROR OCCURRED!")
        print("="*70)
        print(f"Error: {e}")
        print("\nTroubleshooting:")
        print("  1. Make sure you have internet connection")
        print("  2. Try installing/updating one of the supported browsers")
        print("  3. Check if antivirus is blocking browser automation")
        print("="*70 + "\n")
        
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    test_scraper()
