"""
Integration Example: Scrape and Analyze LinkedIn Posts

This script demonstrates how to combine the LinkedIn scraper with the post analyzer
to extract post data and perform comprehensive text analysis.
"""

import json
import sys
from pathlib import Path
from typing import Dict, Any

# Add scripts directory to path
sys.path.append(str(Path(__file__).parent))

from post_analyzer import extract_post_features, analyze_engagement_signals

# Try to import engagement predictor
try:
    from engagement_predictor import predict_engagement_score
    PREDICTOR_AVAILABLE = True
except ImportError:
    PREDICTOR_AVAILABLE = False


def load_scraped_post(json_path: str = "data/json/post_data.json") -> Dict[str, Any]:
    """
    Load scraped post data from JSON file.
    
    Args:
        json_path (str): Path to the JSON file containing scraped data
        
    Returns:
        Dict[str, Any]: Scraped post data
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def analyze_scraped_post(json_path: str = "data/json/post_data.json") -> Dict[str, Any]:
    """
    Load and analyze a scraped LinkedIn post.
    
    Args:
        json_path (str): Path to the JSON file containing scraped data
        
    Returns:
        Dict[str, Any]: Combined scraped data and analysis results
    """
    # Load scraped data
    post_data = load_scraped_post(json_path)
    
    # Extract post text
    post_text = post_data.get('post_text', '')
    
    if not post_text:
        print("‚ö†Ô∏è Warning: No post text found in scraped data")
        return {
            'scraped_data': post_data,
            'text_features': {},
            'engagement_analysis': {}
        }
    
    # Analyze post features
    features = extract_post_features(post_text)
    
    # Analyze engagement signals
    engagement = analyze_engagement_signals(features)
    
    # Predict engagement performance if predictor available
    prediction = None
    if PREDICTOR_AVAILABLE:
        sentiment_data = features.get('sentiment')
        prediction = predict_engagement_score(post_data, sentiment_data, features)
    
    # Combine all data
    result = {
        'scraped_data': post_data,
        'text_features': features,
        'engagement_analysis': engagement,
        'engagement_prediction': prediction
    }
    
    return result


def print_analysis_report(analysis: Dict[str, Any]) -> None:
    """
    Print a formatted analysis report.
    
    Args:
        analysis (Dict[str, Any]): Analysis results from analyze_scraped_post()
    """
    scraped = analysis['scraped_data']
    features = analysis['text_features']
    engagement = analysis['engagement_analysis']
    
    print("\n" + "="*80)
    print("LINKEDIN POST ANALYSIS REPORT")
    print("="*80)
    
    # Post metadata
    print("\nüìÑ POST METADATA:")
    print("-"*80)
    print(f"URL: {scraped.get('url', 'N/A')}")
    print(f"Author: {scraped.get('author', 'N/A')}")
    print(f"Browser Used: {scraped.get('browser_used', 'N/A')}")
    print(f"Extracted At: {scraped.get('extracted_at', 'N/A')}")
    
    # Engagement metrics
    print("\nüìä ENGAGEMENT METRICS:")
    print("-"*80)
    likes = scraped.get('likes') or 0
    comments = scraped.get('comments') or 0
    shares = scraped.get('shares') or 0
    
    print(f"Likes: {likes:,}" if likes else "Likes: N/A")
    print(f"Comments: {comments:,}" if comments else "Comments: N/A")
    print(f"Shares: {shares:,}" if shares else "Shares: N/A")
    
    total_engagement = likes + comments + shares
    if total_engagement > 0:
        print(f"Total Engagement: {total_engagement:,}")
    else:
        print("Total Engagement: N/A")
    
    # Text features
    print("\nüìù TEXT ANALYSIS:")
    print("-"*80)
    if features:
        print(f"Word Count: {features.get('word_count', 0)}")
        print(f"Character Count: {features.get('character_count', 0)}")
        print(f"Average Word Length: {features.get('average_word_length', 0)}")
        print(f"Unique Words: {features.get('unique_words', 0)}")
        print(f"Sentences: {features.get('sentence_count', 0)}")
        print(f"Lines: {features.get('line_count', 0)}")
    else:
        print("No text analysis available (post text is empty)")
    
    # Content elements
    print("\nüè∑Ô∏è CONTENT ELEMENTS:")
    print("-"*80)
    if features:
        print(f"Hashtags ({features.get('hashtag_count', 0)}): {', '.join(features.get('hashtags', [])) if features.get('hashtags') else 'None'}")
        print(f"Mentions ({features.get('mention_count', 0)}): {', '.join(features.get('mentions', [])) if features.get('mentions') else 'None'}")
        print(f"URLs ({features.get('url_count', 0)}): {features.get('url_count', 0)} found")
        print(f"Emojis ({features.get('emoji_count', 0)}): {' '.join(features.get('emojis', [])) if features.get('emojis') else 'None'}")
        print(f"Call-to-Action: {'‚úÖ Yes' if features.get('has_call_to_action') else '‚ùå No'}")
        if features.get('call_to_action_phrases'):
            print(f"  CTA Phrases: {', '.join(features['call_to_action_phrases'])}")
    else:
        print("No content elements available (post text is empty)")
    
    # Sentiment analysis
    if features and features.get('sentiment'):
        sentiment = features['sentiment']
        print("\nüòä SENTIMENT ANALYSIS:")
        print("-"*80)
        
        # Display sentiment with emoji indicators
        sentiment_emoji = {
            'very positive': 'üòÑ',
            'positive': 'üôÇ',
            'neutral': 'üòê',
            'negative': 'üòü',
            'very negative': 'üò†'
        }
        emoji = sentiment_emoji.get(sentiment['intensity'], 'üòê')
        
        print(f"Overall Sentiment: {emoji} {sentiment['sentiment_label'].upper()} ({sentiment['intensity']})")
        print(f"Compound Score: {sentiment['compound_score']} (range: -1 to +1)")
        print(f"  ‚Ä¢ Positive: {sentiment['positive_score']:.1%}")
        print(f"  ‚Ä¢ Neutral:  {sentiment['neutral_score']:.1%}")
        print(f"  ‚Ä¢ Negative: {sentiment['negative_score']:.1%}")
        print(f"Confidence: {sentiment['confidence'].title()}")
        print(f"\nüí≠ {sentiment['interpretation']}")
    
    # Engagement analysis
    print("\nüéØ ENGAGEMENT ANALYSIS:")
    print("-"*80)
    if engagement:
        print(f"Engagement Score: {engagement.get('engagement_score', 0)}/100")
        print(f"Readability: {engagement.get('readability_score', 'N/A')}")
        print(f"Strong Hook: {'‚úÖ Yes' if engagement.get('has_strong_hook') else '‚ùå No'}")
        
        if engagement.get('recommendations'):
            print(f"\nüí° RECOMMENDATIONS:")
            for i, rec in enumerate(engagement['recommendations'], 1):
                print(f"  {i}. {rec}")
    else:
        print("No engagement analysis available (post text is empty)")
    
    # Engagement prediction
    prediction = analysis.get('engagement_prediction')
    if prediction:
        print("\nüîÆ ENGAGEMENT PREDICTION:")
        print("-"*80)
        
        # Display main prediction
        label_emoji = {
            'High Performer': 'üöÄ',
            'Medium': 'üìä',
            'Low': 'üìâ'
        }
        emoji = label_emoji.get(prediction['prediction_label'], 'üìä')
        
        print(f"Performance Forecast: {emoji} {prediction['prediction_label']}")
        print(f"Predicted Score: {prediction['engagement_score']}/100 ({prediction['percentile']})")
        print(f"  ‚Ä¢ Base Score (actual engagement): {prediction['base_score']:.1f}/60")
        print(f"  ‚Ä¢ Quality Bonus (content features): {prediction['quality_bonus']:.1f}/40")
        print(f"Confidence: {prediction['confidence_level'].title()} ({prediction['confidence_score']:.0f}%)")
        
        # Display score breakdown
        if prediction.get('breakdown'):
            eng_breakdown = prediction['breakdown'].get('engagement', {})
            print(f"\nWeighted Engagement: {eng_breakdown.get('weighted_total', 0):.1f}")
            print(f"  ‚Ä¢ Likes √ó 1.0 = {eng_breakdown.get('likes', 0)}")
            print(f"  ‚Ä¢ Comments √ó 2.0 = {eng_breakdown.get('comments', 0) * 2}")
            print(f"  ‚Ä¢ Shares √ó 3.0 = {eng_breakdown.get('shares', 0) * 3}")
        
        # Display top recommendations
        if prediction.get('recommendations'):
            print(f"\nüéØ OPTIMIZATION RECOMMENDATIONS:")
            for i, rec in enumerate(prediction['recommendations'][:5], 1):
                print(f"  {i}. {rec}")
    
    # Post preview
    print("\nüìñ POST PREVIEW:")
    print("-"*80)
    preview = scraped.get('post_text') or 'N/A'
    if preview != 'N/A' and len(preview) > 200:
        preview = preview[:200] + "..."
    print(preview)
    
    print("\n" + "="*80)


def save_analysis(analysis: Dict[str, Any], output_path: str = "data/json/post_analysis.json") -> None:
    """
    Save analysis results to JSON file.
    
    Args:
        analysis (Dict[str, Any]): Analysis results
        output_path (str): Path to save the JSON file
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)
    print(f"\n‚úÖ Analysis saved to: {output_path}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Analyze a scraped LinkedIn post")
    parser.add_argument(
        "--input",
        default="data/json/post_data.json",
        help="Path to scraped post JSON file"
    )
    parser.add_argument(
        "--output",
        default="data/json/post_analysis.json",
        help="Path to save analysis results"
    )
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="Don't save results to file"
    )
    
    args = parser.parse_args()
    
    try:
        print("üîç Analyzing scraped LinkedIn post...")
        
        # Perform analysis
        analysis = analyze_scraped_post(args.input)
        
        # Print report
        print_analysis_report(analysis)
        
        # Save results
        if not args.no_save:
            save_analysis(analysis, args.output)
        
        print("\n‚úÖ Analysis complete!")
        
    except FileNotFoundError:
        print(f"\n‚ùå Error: File not found - {args.input}")
        print("Please run the scraper first to generate post data.")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Error: {str(e)}")
        sys.exit(1)
